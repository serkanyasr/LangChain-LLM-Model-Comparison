import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
from langchain_openai import ChatOpenAI

def ask_gpt(prompt, temperature, max_tokens):

    """
    Sends a prompt to the GPT-3.5 Turbo model and returns the AI response.

    Parameters:
    prompt (str): The input prompt to send to the GPT-3.5 Turbo model.
    temperature (float): The temperature parameter controls the randomness of the output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
    max_tokens (int): The maximum number of tokens in the response. This parameter can be used to limit the length of the generated text.

    Returns:
    str: The AI response generated by the GPT-3.5 Turbo model.
    """

    llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=temperature, max_tokens=max_tokens, model="gpt-3.5-turbo")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content



GOOGLE_AI_API_KEY = os.getenv("GOOGLE_AI_API_KEY")
from langchain_google_genai import ChatGoogleGenerativeAI

def ask_gemini(prompt, temperature):

    """
    Sends a prompt to the Gemini AI model and returns the response.

    Args:
        prompt (str): The input prompt to send to the AI model.
        temperature (float): The temperature parameter for controlling the randomness of the AI's response.

    Returns:
        str: The response generated by the AI model.
    """

    llm = ChatGoogleGenerativeAI(google_api_key=GOOGLE_AI_API_KEY, temperature=temperature, model="gemini-pro")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content



ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
from langchain_community.chat_models import ChatAnthropic

def ask_claude(prompt, temperature, max_tokens):

    """
    Sends a prompt to the Claude-2.1 model and returns the response.

    Args:
        prompt (str): The input prompt for the model.
        temperature (float): The temperature parameter for controlling the randomness of the model's output.
        max_tokens (int): The maximum number of tokens in the generated response.

    Returns:
        str: The response generated by the Claude-2.1 model.
    """

    llm = ChatAnthropic(anthropic_api_key=ANTHROPIC_API_KEY, temperature=temperature, max_tokens=max_tokens, model_name="claude-2.1")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content


COHERA_API_KEY = os.getenv("COHERA_API_KEY")
from langchain_community.chat_models import ChatCohere

def ask_command(prompt, temperature, max_tokens):

    """
    Sends a prompt to the ChatCohere model to generate a response.

    Args:
        prompt (str): The input prompt for the model.
        temperature (float): Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more deterministic.
        max_tokens (int): The maximum number of tokens in the generated response.

    Returns:
        str: The generated response from the ChatCohere model.
    """

    llm = ChatCohere(cohere_api_key=COHERA_API_KEY, temperature=temperature, max_tokens=max_tokens, model="command")
    AI_Response = llm.invoke(prompt)
    return AI_Response.content